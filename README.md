# Smart Code Planner ğŸ¤–

A sophisticated code organization agent built with LangGraph that helps developers break down complex tasks into manageable subtasks and provides intelligent code organization advice.

## âœ¨ Features

- **ğŸ§  Intelligent Task Decomposition**: Uses LangGraph workflows to recursively break down complex tasks
- **ğŸ¤– Multi-LLM Support**: Supports both OpenAI (GPT) and Google (Gemini) models
  - OpenAI: `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, `gpt-4`, `gpt-3.5-turbo`
  - Google: `gemini-2.0-flash-lite`, `gemini-2.0-flash`, `gemini-1.5-pro`, `gemini-1.5-flash`, `gemini-pro`
- **ğŸ“Š Visual Workflow**: Terminal graph visualization showing LangGraph structure on startup
- **ğŸ¯ Recursive Analysis**: Automatically identifies when subtasks need further breakdown
- **ğŸ—ï¸ Code Organization Advice**: Provides expert recommendations for project structure
- **ğŸŒ Modern Web UI**: Clean Streamlit interface with model selection and configuration
- **ğŸ³ Docker Support**: Fully containerized for easy deployment
- **ğŸ§ª Comprehensive Testing**: Unit tests, integration tests, and validation scripts
- **ğŸ“– Rich Documentation**: Detailed setup guides and examples

## ğŸ—ï¸ Architecture

The project follows a clean architecture pattern:

```
smart-code-planner/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph.py          # LangGraph workflow definition
â”‚   â”‚   â”œâ”€â”€ nodes.py          # Graph node implementations
â”‚   â”‚   â””â”€â”€ state.py          # State management
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py         # Pydantic models
â”‚   â”‚   â””â”€â”€ config.py         # Configuration management
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ task_analyzer.py  # Task analysis logic
â”‚   â”‚   â””â”€â”€ code_advisor.py   # Code organization advisor
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ streamlit_app.py  # Streamlit interface
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ task_decomposition.txt
â”‚   â”œâ”€â”€ subtask_analysis.txt
â”‚   â””â”€â”€ code_organization.txt
â”œâ”€â”€ tests/
â”œâ”€â”€ Dockerfile              # Docker configuration
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ docker-compose.yml      # Optional Docker Compose setup
```

## ğŸš€ Quick Start

### Using Poetry (Recommended)

1. **Clone and setup**:
   ```bash
   git clone <your-repo-url>
   cd smart-code-planner
   poetry install
   ```

2. **Set environment variables**:
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

3. **Run the application**:
   ```bash
   poetry run streamlit run src/ui/streamlit_app.py
   ```

### Using Docker

1. **Build the Docker image**:
   ```bash
   docker build -t smart-code-planner .
   ```

2. **Run the container**:
   ```bash
   docker run -p 8501:8501 smart-code-planner
   ```

3. **Access the application**:
   Open http://localhost:8501 in your browser

**Alternative with Docker Compose (if available)**:
```bash
docker-compose up --build
```

**Useful Docker commands**:
```bash
# Run in background
docker run -d -p 8501:8501 --name smart-code-planner-app smart-code-planner

# View logs
docker logs smart-code-planner-app

# Stop container
docker stop smart-code-planner-app

# Remove container
docker rm smart-code-planner-app
```

## ğŸ”§ Configuration

Create a `.env` file with your API keys:

```env
# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Google API Configuration (for Gemini models)
GOOGLE_API_KEY=your_google_api_key_here

# LangChain Configuration (Optional)
LANGCHAIN_API_KEY=your_langchain_api_key_here
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=smart-code-planner

# Model Configuration
MODEL_NAME=gpt-4o
MODEL_PROVIDER=openai
TEMPERATURE=0.3
MAX_TOKENS=2000
```

### Supported Models

**OpenAI Models:**
- `gpt-4o` (recommended)
- `gpt-4o-mini`
- `gpt-4-turbo`
- `gpt-4`
- `gpt-3.5-turbo`

**Google Gemini Models:**
- `gemini-2.0-flash-lite` (fast and efficient)
- `gemini-2.0-flash` (recommended)
- `gemini-1.5-pro`
- `gemini-1.5-flash`
- `gemini-pro`

## ğŸ“– Usage

1. **Launch the Streamlit interface** - The app will display the LangGraph workflow visualization in the terminal
2. **Select your preferred AI model** from the sidebar (OpenAI or Google Gemini)
3. **Configure analysis parameters** (depth, temperature, etc.)
4. **Enter your coding task** in the text area or choose from examples
5. **Click "Analyze Task"** to start the decomposition process
6. **Review the comprehensive results**:
   - Task breakdown with complexity analysis
   - Recursive subtask decomposition
   - Code organization recommendations
   - Implementation roadmap
7. **Export results** to JSON or Markdown for further use

## ğŸ§ª Testing

Run the test suite:

```bash
poetry run pytest
```

Run with coverage:

```bash
poetry run pytest --cov=src tests/
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [LangGraph](https://github.com/langchain-ai/langgraph) for the graph-based workflow engine
- [Streamlit](https://streamlit.io/) for the beautiful web interface
- [Poetry](https://python-poetry.org/) for dependency management

---

**Built with â¤ï¸ by Federico Antosiano**
